<!DOCTYPE html>

<head>
    <link rel="stylesheet" type="text/css" href="paper.css">
    <title>HTTP 2 explained</title>
</head>

<body>
    <!-- Side navigation -->
    <div class="sidenav">
        <a href="#top">1. Какво е HTTP?</a>
        <a href="#flaws_of_http1x">2. Недостатъците на HTTP/1.x</a>
        <a href="#what_is_http2">3. Какво е HTTP/2?</a>
        <a href="#binary_framing_layer">4. Binary framing layer</a>
        <a href="#streams_messages_frames">5. Streams, messages, and frames</a>
        <a href="#request_response_multiplexing">6. Request and response multiplexing</a>
        <a href="#stream_prioritization">7. Stream prioritization</a>
        <a href="#one_tcp_connection_per_origin">8. One TCP connection per origin</a>
        <a href="#flow_control">9.Flow control</a>
        <a href="#server_push">10. Server push</a>
        <a href="#push_promise_101">11. PUSH_PROMISE 101</a>
        <a href="#header_compression">12. Header compression</a>
        <a href="#resources">Resources</a>
    </div>

    <div class="main">
        <b>
            <p id="top">
                <i>
                    Преди да започнете четенето на реферата, искам да кажа, че на места съм избягвал превод на основните
                    термини, свързани с HTTP/2, понеже ми звучат неестествено, а и не мисля, че има много смисъл. Както
                    гласи една поговорка:
                    <br>
                    - Преводите са като жените - ако са хубави, не са вярни, а ако са вярни, не са хубави.
                    <br><br>
                    (Дано рецензията ми не я прави жена...)
                </i>
            </p>
        </b>

        <h1 id="what_is_http">1. Какво е HTTP? </h1>

        <img src="images/HTTP-Protocol.png" alt="HTTP request and response">

        <p>HTTP е мрежов протокол от приложния слой на OSI модела, служещ за обмяна на информация и ресурси в компютърни
            мрежи. Доказва се като много успешен през годините. С времето обаче се оказва, че начинът, по който
            по-старите
            версии на HTTP (1.0, 1.1) използват транспортния слой под себе си, е неефикасен и не отговаря на нуждите на
            модерния свят. Казано по-просто, мрежата, по която се пренасят HTTP съобщенията се натоварва излишно много и
            това води до проблеми.
        </p>

        <h1 id="flaws_of_http1x">2. Какви всъщност са недостатъците на HTTP/1.1 и HTTP/1.0? </h1>

        <p>За да бъда по-конкретен, нека да се върнем малко в историята.<br>
            Първите версии на HTTP са били създадени, търсейки простота. За жалост, тази простота е била за сметка на
            производителността на приложенията. Какви всъщост са проблемите?
        </p>

        <ul>
            <li>HTTP/1.0 позволява само и единствено една заявка да преминава през изградена TCP конекция. Това очевидно
                струва доста скъпо - всеки път, когато изпращаме заявка, трябва да преминаваме през изграждане на TCP
                връзка
                - т.нар. three-way handshake. <br>
                HTTP/1.1 частично се справи с този проблем чрез <b><a
                        href="https://en.wikipedia.org/wiki/HTTP_pipelining">HTTP pipelining</a></b>. Накратко това ни
                позволява да изпращаме
                няколко заявки през една и съща TCP конекция (както се вижда на втората снимка - заявката на HTTP/1.1
                изисква снимка на къщата и колата, преди да се сложи край на конекцията). Все още обаче го има проблема
                с
                <b><a href="https://en.wikipedia.org/wiki/Head-of-line_blocking">Head-of-line blocking (HOL
                        blocking)</a></b>.

                <p>
                    Заради това клиентите, използващи HTTP/1.x, които искат да изпращат <b>много</b> заявки към
                    даден сървър, са принудени да изграждат няколко TCP конекции към сървъра, за да постигнат
                    конкурентност
                    на заявките (търсейки намаляване на <a href="http://www.linfo.org/latency.html">латенцията</a>).
                </p>
            </li>

            <img src="images/http2-changes-everything-4-638.jpg" alt="HTTP: 1.1 vs 1.0" height="450" width="650">
            <img src="images/http2-changes-everything-5-638.jpg" alt="HTTP: 1.1 vs 1.0" height="450" width="650">

            <li>
                HTTP хедърите (header fields) доста често са многословни и повтарящи се, което причинява ненужно
                натоварване на мрежовия трафик, както и препълване на т.нар.
                <a href="https://en.wikipedia.org/wiki/TCP_congestion_control#Congestion_window">TCP congestion
                    window</a> доста бързо.
            </li>
        </ul>

        <p>
            Тези проблеми не са били фатални, но с времето уеб приложенията са започнали да разширяват обхвата си, да
            стават
            по-сложни, както и по-важни за нас, хората. По-този начин отежниха задачата на самите
            потребителите на уеба, както и на уеб разработчиците. Точно това се опитва да олекоти HTTP/2.
        </p>

        <p>
            HTTP/2 се справя с тези проблеми, без да променя семантиката на протокола. Просто оптимизира начина, по
            който
            съобщенията се пренасят по транспортния слой/мрежата. Оптимизаците включват <b>мултиплексиране</b> на заявки
            и
            отговори (requests and responses) в една TCP конекция посредством т.нар. <b>streams</b>, както и
            <b>приоритизирането</b> на заявки - това позволява
            по-важните заявки да приключват по-бързо, което в крайна сметка подобрява производителността.
            <br><br>
            В крайна сметка HTTP/2 щади повече мрежата, защото се използват по-малко TCP конекции в сравнение с
            HTTP/1.x.

            Също така HTTP/2 обработва съобщенията по-ефикасно, използвайки т.нар. <b>binary framing layer</b>
        </p>

        <img src="images/http2-changes-everything-13-638.jpg" alt="HTTP timeline" height="450">

        <h1 id="what_is_http2">3. Какво е HTTP/2? </h1>

        <p>
            HTTP/2 е оптимизация на HTTP/1.1, позволяваща по-добро използване на мрежовите ресурси. Основната цел му е
            чрез
            нови способи за пренос на данни по мрежата, да се ускори времето за зареждане на дадена уеб страница.
            Предимствата на HTTP/2 са неща като по-добро управление на връзката, ниско потребление на трафик,
            бързодействие.
            Семантично по никакъв начин не се различава от HTTP/1.1. Всички основни концепции, като HTTP methods, status
            codes, URIs, headers fields, си остават същите.
            Разликата е в това как самите данни се форматират и пренасят между клиента и сървъра.
            <br>В резултат на това всички приложения, които
            използват HTTP/1.1 могат да преминат към HTTP/2 без модификации.

            <h2>Основни начини, по които HTTP 2 постига по-добра ефикасност:</h2>
            <ul>
                <li>Request and response multiplexing in one TCP connection</li>
                <li>Compression of HTTP header fields</li>
                <li>Support for request prioritization</li>
                <li>Server push</li>
            </ul>

            <p>За да могат да се имплементират изброените по-горе feature-и, са използвани много подобрения на самия
                протокол - нов начин за flow control, error handling и upgrade mechanisms. Но гореописаните 4 неща са
                най-важните, които трябва да бъдат разбрани.
            </p>

        </p>

        <h1 id="binary_framing_layer">4. Binary framing layer</h1>

        <p>
            В основата на подобренията на HTTP 2 е споменатият по-горе <b>binary framing layer</b>, който определя как
            съобщенията
            се изграждат и трансферират между клиента и сървъра.
        </p>

        <img src="images/binary_framing_layer01.svg" alt="binary framing layer" height="350" width="700">

        <p>
            Думата "layer" (слой) се отнася до избора да се използва нов <b>механизъм за кодиране на съобщенията</b>
            между
            socket interface-а и
            HTTP API-то, с което нашите приложения работят. Семантиката на HTTP - verbs, methods и headers, не е
            засегната,
            но начинът, по който
            се кодира, докато се пренася по мрежата е различен. За разлика от HTTP/1.x, където имаме обикновен текст
            разделен с нови редове,
            всичката комуникация в HTTP/2 е разделена на по-малки messages и frames, като всяко от тях е кодирано в
            двоичен
            (binary) формат.
            <br><br>
            В резултат на този механизъм за кодиране, и клиентът, и сървърът трябва да 'знаят' и използват този
            механизъм,
            за да могат да се
            разбират. HTTP/1.x клиент няма да може да се разбере със сървър, поддържащ единствено HTTP/2, както и
            обратното.
            За щастие, нашите уеб
            приложения изобщо не е нужно да разбират за всички тези промени, тъй като HTTP клиентите и сървърите вършат
            работата 'зад завесите'.
        </p>

        <h1 id="streams_messages_frames">5. Streams, messages, and frames</h1>

        <p>
            Появата на <b>binary framing</b> механизма променя начина, по който данните меуду клиента и сървъра се
            обменят. За да опишем този процес, нека първо да се запознаем с HTTP/2 терминологията:
        </p>
        <ul>
            <li><b>Stream (поток)</b> - двупосочен поток от байтове в рамките на изградена връзка (TCP конекция), по
                която може да преминават едно или няколко съобщения (messages).
            </li>
            <br>
            <li>
                <b>Message (съобщение)</b> - поредица от <b>frames</b>, които в съвкупност образуват логическа
                заявка или отговор (logical request or response message).
            </li>
            <br>
            <li>
                <b>Frame</b> - най-малката 'комуникационна единица' в HTTP/2. Всеки <b>frame</b> съдържа т.нар. <u>frame
                    header</u>,
                който идентифицира stream-a, към който принадлежи.
            </li>
        </ul>
        <p>
            Връзката между тези термини може да се сумаризира по следния начин:<br><br>

            - Цялата комуникация между клиента и сървъра се извършва върху една TCP конекция, която има възможността да
            поддържа произволен брой двупосочни потоци (bidirectional streams).
            <br><br>
            - Всеки поток (stream) има уникален идентификатор и опционална информация, задаваща приоритета му,
            използвана за преноса на messages.
            <br><br>
            - Всеки message е логически HTTP message (request или response), което е съставено от един или няколко
            frame-a.
            <br><br>
            - Frame-ът е най-малката 'комуникационна единица', носеща със себе си специфични данни - например HTTP
            headers, message payload и т.н.
            Frame-ове, принадлежащи на <b>различни</b> потоци (streams), могат да бъдат 'преплетени' (interleaved) и
            впоследствие сглобени в
            съобщение, използвайки frame header за да се извлече информация към кой поток принадлежи.
        </p>

        <img src="images/streams_messages_frames01.svg" alt="streams, messages and frames" height="700" width="1000">

        <p>
            Накратко, HTTP/2 разбива комуникацията на HTTP протокола до обмен на <b>binary-encoded frames</b>, които
            сами по себе принадлежат на точно определен stream, и изграждат HTTP message. А всеки stream е
            мултиплексиран
            върху една и съща TCP конекция. Именно това стои в основата на всички други оптимизации, които HTTP/2
            предлага.
        </p>

        <h1 id="request_response_multiplexing">6. Request and response multiplexing</h1>

        <p>
            С HTTP/1.x, ако клиентът иска да направи няколко паралени заявки (request-и) към сървъра, няма друг
            избор освен изграждането на няколко TCP конекции. Това е така, заради
            HTTP/1.x <b>delivery model-ът</b>, който налага ограничението максимум един response да бъде доставян
            в даден момент (response queuing) в рамките на една TCP конекция. Още по-лошото е, че това довежда до вече
            споменатия HOL
            (head-of-line) blocking.
            Това е неефикасно използване на TCP конекцията.
            <br><br>
            Новият <b>binary framing layer</b> в HTTP/2 премахва тези ограничения и дава възможност
            напълно да се мултиплексират request-ите и respons-ите, позволявайки на клиентите и сървирте да разбиват
            HTTP съобщенията на независими един от друг frame-ове, след това да преплитат тези frame-ове и накрая да ги
            сглобяват обратно на другия край на конекцията.
        </p>

        <img src="images/multiplexing01.svg" alt="multiplexing" height="400" width="1000">

        <p>
            Снимката отгоре показва няколко потока (streams) в рамките на една TCP конекция. Клиентът изпраща DATA
            frame към сървъра, докато през това време сървърът препраща преплетена последователност от frames към
            клиента съответно за stream 1 и stream 3. В резултат на това получаваме 3 паралелни потока в действие.
            Способността да се разбие едно HTTP съобщение на независими frame-ове, след това да се преплитат и накрая да
            се сглобяват в съобщение е най-важното подобрение, което предлага HTTP/2. Ползите са много:
        </p>
        <ul>
            <li>
                Паралелно изпращане на request-и без да се блокира нито един request.
            </li>
            <li>
                Паралелно изпращане на response-и без да се блокира нито един response.
            </li>
            <li>
                Използване на <b>една</b> TCP конекиця за доставяне на <b>няколко</b> request-и и respons-и в паралел
                (следствие от горните).
            </li>
            <li>
                Премахване на HTTP/1.x <b>workarounds</b>, които имат за цел оптимизация на производителността (вече не
                са нужни!).
            </li>
            <li>
                По-бързо зареждане на уеб страници, тъй като латенцията е по-малка. (използваме мрежовите
                ресурси по ефикасно).
            </li>
            <li>
                И много други...
            </li>
        </ul>
        <p>
            Накратко новият <b>binary framing layer</b> в HTTP/2 решава проблема с head-of-line blocking, който го има в
            HTTP/1.x и премахва нуждата от множество TCP конекции с цел изграждане на паралелна обработка на request-и и
            response-и. В резултат на това нашите приложения са по-прости, по-бързи и по-евтини за deployment
            (внедряване).
        </p>

        <h1 id="stream_prioritization">7. Stream prioritization</h1>

        <p>
            След като HTTP съобщението се раздели на индивидуални frame-ове, които принадлежат на различни
            strеам-ове и могат да се мултиплексират, има критично значение за производителността редът,
            по който ги 'преплитаме'. За да улесни този процес, HTTP/2 позволява всеки <b>stream</b> да има
            <b>weight (тежест)</b> и <b>dependency (зависимост)</b>.
        </p>

        <ul>
            <li>
                Всеки stream може да има цяло число от 1 до 256, обозначаващо теглото му.
            </li>
            <li>
                Всеки stream може да има експлицитна зависимост от друг stream.
            </li>
        </ul>
        </p>
        <p>
            Комбинацията от зависимости и тегла позволява на клиента да изгради и сподели
            <b>"дърво на зависимост (prioritization tree)"</b>,
            което казва как той би искал да получава response-ите. От своя страна сървърът може да използва тази
            информация с цел да приоритизира обработката на потоците, като контролира колко ресурси ще се заделят
            за даден поток (CPU, memory и др.). След като response-ът за даден поток е готов, сървърът
            настройва мрежата така, че да осигури оптимална доставка на response-и на <b>по-приоритетни</b> потоци към
            клиента.
        </p>

        <img src="images/stream_prioritization01.svg" alt="stream prioritization" height="400" width="1000">

        <p>
            <b>Stream dependency (зависимост между потоци)</b> в HTTP/2 се дефинира, като зависимият поток има
            <i>референция</i> към уникалния идентификатор на потока, от който зависи. Ако такава референция липсва,
            потокът по-подразбиране става зависим от т.нар. "root stream". Ако декларираме зависимост между потоци,
            това индикира, че за parent stream-ът ще бъдат заделяни ресурси по-рано, отколкото за child stream-овете.
            С други думи "Моля, обработете stream D преди stream C" (на картинката се вижда, че C е зависим от D).
            <br><br>

            За потоците, които имат един и същи баща (sibling streams), се заделят ресурси пропорционално на техните
            тегла.
            Например, ако поток А има тежест 12, а неговия брат B има тежест 4, тогава:
            <ol>
                <li>
                    Сумираме теглата:
                    <code translate="no" dir="ltr">4 + 12 = 16</code>
                </li>
                <li>
                    Делим тежестта на всеки поток на сумата: A = 12/16, B= 4/16, т.е. А получава 3/4, а B получава 1/4
                    от наличните ресурси.
                </li>
            </ol>
            <p>Да разгледаме примерите от снимката по-горе. От ляво на дясно:</p>
            <ol>
                <li>
                    Нито поток А, нито поток B имат зависимост, за това имплицитно са зависими от "root stream". Заради
                    това
                    гледаме теглата (както по-горе обяснихме, А получава 3/4, а B 1/4).
                </li>
                <li>
                    Поток D е зависим от "root stream". C е зависим от D. Така поток D трябва да получи 100% от
                    ресурсите преди поток C.
                    В случая тежестите нямат значение, понеже зависимостта на C играе по-силна роля от тежестта.
                </li>
                <li>
                    Поток D трябва да получи 100% от ресурсите преди поток C. C трябва да получи 100% от ресурсите преди
                    A и B.
                    Поток B трябва да получи 1/3 от ресурсите спрямо поток А.
                </li>
                <li>
                    Поток D трябва да получи 100% от ресурсите преди потоците C и Е; Е и C трябва да получат еднакакви
                    ресурси преди А и B; А и B трябва да получат ресурси пропорционални на теглата им.
                </li>
            </ol>

            <p>
                Както и горните примери показват, комбинацията между тегла и зависимости на потоците са добър и
                експресивен начин да приоритизраме ресурсите, което е критично важно за производителността.
                Също така HTTP/2 позволява на клиента да ъпдейтва тези предпочитания по всяко едно време, което
                позволява да се правят още оптимизации в browser-a. С други думи, можем да променяме теглата и
                зависимостите спрямо действията на потребителя, например.
            </p>

            <p class="note">
                <strong>Забележка:</strong><span>
                    Зависимостите и теглата са <i>предпочитание</i> за транспрот, а не <i>изискване</i>.
                    Затова те не гарантират обработка или изпращане на ресурсите по определен ред. Тоест клиентът не
                    може да
                    'насили' сървъра да обработва потока по определен ред използвайки приоритета на самия поток.
                    Това може да звучи неинтуитивно, но всъщност е поведение, което желаем. Защото не искаме сървърът да
                    блокира и да не прави никакъв прогрес върху ресурс с <b>по-нисък</b> приоритет, ако случайно някой
                    ресурс с <b>по-висок</b> приоритет бъде блокиран.
                </span>
            </p>
        </p>

        <h1 id="one_tcp_connection_per_origin">8. One TCP connection per origin</h1>

        <p>
            С новия <b>binary framing layer</b>, HTTP/2 няма нужда от изграждането на няколко TCP конекции.
            Както разбрахме, всеки stream бива 'преплетен' и приоритизиран. В резултат на това, всички HTTP/2
            конекции са постоянни (persistent) и само една конекция от дадена изходна точка е нужна. Това подобрява
            многократно производителността.

            Повечето HTTP размени на съобщения са кратки и бързи, докато TCP е създаден за дълготрайни и
            огромни трансфери на данни. Използвайки една TCP конекция, HTTP/2 може да използва по-ефикасно
            всяка една TCP конекция и също така значително да намали цялостния <b>overhead</b>, свързан с протокола.
            Още повече, използването на по-малко конекции намалява използваната памет и изчислителни ресурси.
        </p>

        <p class="note">
            <strong>Забележка:</strong><span>
                По-малкия брой конекции е изкючително важен feature за подобряването на производителността на
                deployments,
                разчитащи на <a href="https://en.wikipedia.org/wiki/HTTPS">HTTPS</a>,
                защото ще има по-малък брой TLS handshakes, които са скъпи, както и по-добро преизползване на дадена
                сесия.
            </span>
        </p>
        </p>

        <h1 id="flow_control">9.Flow control</h1>
        <p>
            Flow control е механизъм, с който получателят на дадени ресурси може да каже "Чакай! Спри! Много бързо ми
            изпращаш информацията и
            аз нямам възможност да я обработвам. Забави темпото". С други думи, този, който получава HTTP съобщенията
            може да:
        </p>
        <ul>
            <li>е зает</li>
            <li>е подложен на голям трафик</li>
            <li>няма възможност да задели дадено количество ресурси за даден stream</li>
        </ul>

        <p>Например: </p>
        <ul>
            <li>
                Клиентът може да е изпратил заявка за огромен video stream с висок приоритет,
                но потребителят да е цъкнал pause на видеото. В този момент клиентът иска да спре или намали
                потокът от данни към себе си, с цел избягване на пренос и буфериране на излишни данни.
            </li>
            <li>
                Даден proxy server може да има бързи downstream и бавни upstream конекции и иска да регулира
                колко бързо downstream конекциите пренасят данните, за да ги синхронизира с бързината на upstream
                конекциите, с цел контролиране на използваните ресурси.
            </li>
            <li>
                И други...
            </li>
        </ul>

        <p>
            Горните неща може би ви напомнят за <a
                href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol#Flow_control">TCP flow control</a> -
            проблемът там е идентичен. Разликата обаче е, че HTTP/2 потоците се мултиплексират върху <u>една</u> TCP
            конекция,
            което затруднява регулацията на тяхния пренос.
            <br>
            HTTP/2 предоставя няколко прости елемента, които позволяват на клиента и сървъра да си имплементират сами
            <b>stream- и connection-level flow control:</b>
        </p>

        <ul>
            <li>
                Всеки получател може да си избере сам <b>window size-ът</b>, който желае за отделните streams, както и
                за цялата конекция.
            </li>
            <li>
                Flow control не може да бъде деактивиран. Когато HTTP/2 конекцията е изградена, клиентът и сървърът си
                разменят
                <b>SETTINGS</b> frames, които определят големината на <b>flow control windows</b> в двете посоки на
                комуникацията.
                Стойността по подразбиране на този window е 65,535 байта, но получателят може да го set-не на
                максималната стойност (2^31-1 байта),
                и да го поддържа изпращайки <b>WINDOW_UPDATE</b> frame, когато получи някакви данни.
            </li>
            <li>
                Flow control е базиран на <b>hop-by-hop</b>, а не на <b>end-to-end</b>. Тоест всички машини, през които
                минават
                данните, докато отиват към крайната цел, могат да влияят на flow-control-ът (за разлика от end-to-end,
                където не ни интересува изобщо
                през къде минават данните).
            </li>
        </ul>


        <p>
            HTTP/2 не използва специфичен алгоритъм за имплементацията на flow control. Той предоставя елементи, чрез
            които клиентът и сървърът да могат сами да си избератстратегията и алгоритъма, с който ще го имплементират.
            Това може да подобри както истинската производителност, така и
            <a href="https://hpbn.co/primer-on-web-performance/#speed-performance-and-human-perception">възприетата от
                човека производителност</a>.
            <br><br>
            Например application-layer flow control може да позволи на browser-а, ако иска, да си fetch-не само част от
            определен ресурс.
            След това ще отложи (on hold) доставката на оставащата част този ресурс, намалявайки stream flow control
            window-ът на 0. И по-късно може
            да го увеличи. С други думи, browser-ът например може да си изтегли само preview на дадена картина, да я
            покаже на потребителя и
            после да позволи на други ресурси с висок приоритет да бъдат fetch-нати. По-късно, след като по-критичните
            ресурси бъдат изтеглени, ще си доизтегли цялата картинка.
        </p>

        <h1 id="server_push">10. Server push</h1>
        <p>Another powerful new feature of HTTP/2 is the ability of the server to send multiple responses for a single
            client request. That is, in addition to the response to the original request, the server can push additional
            resources to the client, without the client having to request each one explicitly.
        </p>

        <img src="images/push01.svg" alt="server push" height="400" width="1000">

        <p>
            Note: HTTP/2 breaks away from the strict request-response semantics and enables one-to-many and
            server-initiated
            push workflows that open up a world of new interaction possibilities both within and outside the browser.
            This
            is an enabling feature that will have important long-term consequences both for how we think about the
            protocol,
            and where and how it is used.
            Why would we need such a mechanism in a browser? A typical web application consists of dozens of resources,
            all
            of which are discovered by the client by examining the document provided by the server. As a result, why not
            eliminate the extra latency and let the server push the associated resources ahead of time? The server
            already
            knows which resources the client will require; that’s server push.

            In fact, if you have ever inlined a CSS, JavaScript, or any other asset via a data URI (see Resource
            Inlining),
            then you already have hands-on experience with server push. By manually inlining the resource into the
            document,
            we are, in effect, pushing that resource to the client, without waiting for the client to request it. With
            HTTP/2 we can achieve the same results, but with additional performance benefits. Push resources can be:

            Cached by the client
            Reused across different pages
            Multiplexed alongside other resources
            Prioritized by the server
            Declined by the client
        </p>

        <h1 id="push_promise_101">11. PUSH_PROMISE 101</h1>
        <p>
            All server push streams are initiated via PUSH_PROMISE frames, which signal the server’s intent to push the
            described resources to the client and need to be delivered ahead of the response data that requests the
            pushed
            resources. This delivery order is critical: the client needs to know which resources the server intends to
            push
            to avoid creating duplicate requests for these resources. The simplest strategy to satisfy this requirement
            is
            to send all PUSH_PROMISE frames, which contain just the HTTP headers of the promised resource, ahead of the
            parent’s response (in other words, DATA frames).

            Once the client receives a PUSH_PROMISE frame it has the option to decline the stream (via a RST_STREAM
            frame)
            if it wants to. (This might occur for example because the resource is already in cache.) This is an
            important
            improvement over HTTP/1.x. By contrast, the use of resource inlining, which is a popular "optimization" for
            HTTP/1.x, is equivalent to a "forced push": the client cannot opt-out, cancel it, or process the inlined
            resource individually.

            With HTTP/2 the client remains in full control of how server push is used. The client can limit the number
            of
            concurrently pushed streams; adjust the initial flow control window to control how much data is pushed when
            the
            stream is first opened; or disable server push entirely. These preferences are communicated via the SETTINGS
            frames at the beginning of the HTTP/2 connection and may be updated at any time.

            Each pushed resource is a stream that, unlike an inlined resource, allows it to be individually multiplexed,
            prioritized, and processed by the client. The only security restriction, as enforced by the browser, is that
            pushed resources must obey the same-origin policy: the server must be authoritative for the provided
            content.
        </p>

        <h1 id="header_compression">12. Header compression</h1>
        <p>
            Each HTTP transfer carries a set of headers that describe the transferred resource and its properties. In
            HTTP/1.x, this metadata is always sent as plain text and adds anywhere from 500–800 bytes of overhead per
            transfer, and sometimes kilobytes more if HTTP cookies are being used. (See Measuring and Controlling
            Protocol
            Overhead .) To reduce this overhead and improve performance, HTTP/2 compresses request and response header
            metadata using the HPACK compression format that uses two simple but powerful techniques:

            It allows the transmitted header fields to be encoded via a static Huffman code, which reduces their
            individual
            transfer size.
            It requires that both the client and server maintain and update an indexed list of previously seen header
            fields
            (in other words, it establishes a shared compression context), which is then used as a reference to
            efficiently
            encode previously transmitted values.
            Huffman coding allows the individual values to be compressed when transferred, and the indexed list of
            previously transferred values allows us to encode duplicate values by transferring index values that can be
            used
            to efficiently look up and reconstruct the full header keys and values.
        </p>

        <img src="images/header_compression01.svg" alt="header compression" height="400" width="1000">

        <p>
            As one further optimization, the HPACK compression context consists of a static and dynamic table: the
            static
            table is defined in the specification and provides a list of common HTTP header fields that all connections
            are
            likely to use (e.g., valid header names); the dynamic table is initially empty and is updated based on
            exchanged
            values within a particular connection. As a result, the size of each request is reduced by using static
            Huffman
            coding for values that haven’t been seen before, and substitution of indexes for values that are already
            present
            in the static or dynamic tables on each side.

            Note: The definitions of the request and response header fields in HTTP/2 remains unchanged, with a few
            minor
            exceptions: all header field names are lowercase, and the request line is now split into individual :method,
            :scheme, :authority, and :path pseudo-header fields.
        </p>

        <h2 id="resources">RESOURCES:</h2>
        <ul>
            <a href="https://tools.ietf.org/html/rfc7540">
                <li>https://tools.ietf.org/html/rfc7540</li>
            </a>

            <a href="https://en.wikipedia.org/wiki/HTTP/2">
                <li>https://en.wikipedia.org/wiki/HTTP/2</li>
            </a>

            <a
                href="https://developers.google.com/web/fundamentals/performance/http2#a_brief_history_of_spdy_and_http2">
                <li>https://developers.google.com/web/fundamentals/performance/http2#a_brief_history_of_spdy_and_http2
                </li>
            </a>

            <a href="https://en.wikipedia.org/wiki/HTTP_pipelining">
                <li>https://en.wikipedia.org/wiki/HTTP_pipelining</li>
            </a>

            <a href="https://en.wikipedia.org/wiki/Head-of-line_blocking">
                <li>https://en.wikipedia.org/wiki/Head-of-line_blocking</li>
            </a>

        </ul>
    </div>

</body>